{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peterchen/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # 检查 CUDA 是否可用\n",
    "print(torch.cuda.current_device())  # 查看当前选中的 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "from utils import *\n",
    "from config import parameters as conf\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from Model import Bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: operation_list.txt\n",
      "Reading: constant_list.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertTokenizer\n",
    "# 模型架构配置\n",
    "from transformers import BertConfig\n",
    "tokenizer = BertTokenizer.from_pretrained(conf.model_size)\n",
    "model_config = BertConfig.from_pretrained(conf.model_size)\n",
    "\n",
    "import json\n",
    "\n",
    "model_dir_name = conf.model_save_name + \"_\" + \\\n",
    "    datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "model_dir = os.path.join(conf.output_path, model_dir_name)\n",
    "results_path = os.path.join(model_dir, \"results\")\n",
    "saved_model_path = os.path.join(model_dir, \"saved_model\")\n",
    "os.makedirs(saved_model_path, exist_ok=False)\n",
    "os.makedirs(results_path, exist_ok=False)\n",
    "log_file = os.path.join(results_path, 'log.txt')\n",
    "\n",
    "op_list = read_txt(conf.op_list_file, log_file)\n",
    "op_list = [op + '(' for op in op_list]\n",
    "op_list = ['EOF', 'UNK', 'GO', ')'] + op_list\n",
    "const_list = read_txt(conf.const_list_file, log_file)\n",
    "const_list = [const.lower().replace('.', '_') for const in const_list]\n",
    "reserved_token_size = len(op_list) + len(const_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/peterchen/FinQA/dataset/train.json\n",
      "Reading /home/peterchen/FinQA/dataset/dev.json\n",
      "Reading /home/peterchen/FinQA/dataset/test.json\n"
     ]
    }
   ],
   "source": [
    "train_data, train_examples, op_list, const_list = \\\n",
    "read_partial_examples(input_path=conf.train_file, tokenizer=tokenizer,\n",
    "        op_list=op_list, const_list=const_list, log_file=log_file,\n",
    "        max_examples=100)\n",
    "\n",
    "valid_data, valid_examples, op_list, const_list = \\\n",
    "read_partial_examples(input_path=conf.valid_file, tokenizer=tokenizer,\n",
    "        op_list=op_list, const_list=const_list, log_file=log_file,\n",
    "        max_examples=10)\n",
    "\n",
    "test_data, test_examples, op_list, const_list = \\\n",
    "read_partial_examples(input_path=conf.test_file, tokenizer=tokenizer,\n",
    "        op_list=op_list, const_list=const_list, log_file=log_file,\n",
    "        max_examples=10)\n",
    "\n",
    "kwargs = {\"examples\": train_examples,\n",
    "\"tokenizer\": tokenizer,\n",
    "\"option\": conf.option,\n",
    "\"is_training\": True,\n",
    "\"max_seq_length\": conf.max_seq_length,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:08, 12.37it/s]\n",
      "10it [00:00, 12.74it/s]\n",
      "10it [00:00, 12.56it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(**kwargs)\n",
    "kwargs[\"examples\"] = valid_examples\n",
    "kwargs[\"is_training\"] = False\n",
    "valid_features = convert_examples_to_features(**kwargs)\n",
    "kwargs[\"examples\"] = test_examples\n",
    "test_features = convert_examples_to_features(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################INPUT PARAMETERS###################\n",
      "__module__ = config\n",
      "prog_name = retriever\n",
      "root_path = /home/peterchen/FinQA/\n",
      "output_path = /home/peterchen/FinQA/retriever_output/\n",
      "cache_dir = /home/peterchen/FinQA/retriever_output/cache/\n",
      "model_save_name = retriever-bert-base-test\n",
      "train_file = /home/peterchen/FinQA/dataset/train.json\n",
      "valid_file = /home/peterchen/FinQA/dataset/dev.json\n",
      "test_file = /home/peterchen/FinQA/dataset/test.json\n",
      "op_list_file = operation_list.txt\n",
      "const_list_file = constant_list.txt\n",
      "pretrained_model = bert\n",
      "model_size = bert-base-uncased\n",
      "device = cuda\n",
      "mode = train\n",
      "resume_model_path = \n",
      "saved_model_path = /home/peterchen/FinQA/retriever_output/bert-base-6k_20210427232814/saved_model/loads/3/model.pt\n",
      "build_summary = False\n",
      "option = rand\n",
      "neg_rate = 3\n",
      "topn = 5\n",
      "sep_attention = True\n",
      "layer_norm = True\n",
      "num_decoder_layers = 1\n",
      "max_seq_length = 512\n",
      "max_program_length = 100\n",
      "n_best_size = 20\n",
      "dropout_rate = 0.1\n",
      "batch_size = 16\n",
      "batch_size_test = 16\n",
      "epoch = 1\n",
      "learning_rate = 2e-05\n",
      "report = 10\n",
      "report_loss = 10\n",
      "__dict__ = <attribute '__dict__' of 'parameters' objects>\n",
      "__weakref__ = <attribute '__weakref__' of 'parameters' objects>\n",
      "__doc__ = None\n",
      "#######################################################\n"
     ]
    }
   ],
   "source": [
    "write_log(log_file, \"####################INPUT PARAMETERS###################\")\n",
    "for attr in conf.__dict__:\n",
    "    # .__dict__是一个特殊的属性，它包含了一个对象的所有属性和它们的值\n",
    "    value = conf.__dict__[attr]\n",
    "    write_log(log_file, attr + \" = \" + str(value))\n",
    "write_log(log_file, \"#######################################################\")\n",
    "\n",
    "model = Bert_model(hidden_size=model_config.hidden_size,\n",
    "                       dropout_rate=conf.dropout_rate,)\n",
    "# 多个GPU上并行\n",
    "model = nn.DataParallel(model)\n",
    "# 将模型发送到指定的设备\n",
    "model.to(conf.device)\n",
    "optimizer = optim.Adam(model.parameters(), conf.learning_rate)\n",
    "# 使用交叉熵损失函数，忽略标签为-1的样本\n",
    "criterion = nn.CrossEntropyLoss(reduction='none', ignore_index=-1)\n",
    "model.train()\n",
    "\n",
    "train_iterator = DataLoader(\n",
    "        is_training=True, data=train_features, batch_size=conf.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train iter length 44\n"
     ]
    }
   ],
   "source": [
    "# 初始化用于记录训练进度和性能的变量，如批次计数器、损失记录等\n",
    "k = 0\n",
    "record_k = 0\n",
    "record_loss_k = 0\n",
    "loss, start_time = 0.0, time.time()\n",
    "record_loss = 0.0\n",
    "\n",
    "print(\"train iter length\", len(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'input_mask', 'segment_ids', 'filename_id', 'label', 'ind'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for first_batch in train_iterator:\n",
    "    break\n",
    "\n",
    "first_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_1',\n",
       " 'table_2',\n",
       " 'text_15',\n",
       " 'table_1',\n",
       " 'table_1',\n",
       " 'table_8',\n",
       " 'table_1',\n",
       " 'table_6',\n",
       " 'table_1',\n",
       " 'table_2',\n",
       " 'text_0',\n",
       " 'text_1',\n",
       " 'table_1',\n",
       " 'table_15',\n",
       " 'table_1',\n",
       " 'text_7']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['ind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_ori, data, model, ksave_dir, mode='valid'):\n",
    "    # 评估模型在验证集（或测试集）上的性能\n",
    "    # ksave_dir: 保存目录\n",
    "\n",
    "    pred_list = []\n",
    "    pred_unk = []\n",
    "\n",
    "    ksave_dir_mode = os.path.join(ksave_dir, mode)\n",
    "    os.makedirs(ksave_dir_mode, exist_ok=True)\n",
    "\n",
    "    data_iterator = DataLoader(\n",
    "        is_training=False, data=data, batch_size=conf.batch_size_test, shuffle=False)\n",
    "\n",
    "    k = 0\n",
    "    all_logits = []\n",
    "    all_filename_id = []\n",
    "    all_ind = []\n",
    "    with torch.no_grad():\n",
    "        # 关闭梯度计算\n",
    "        for x in tqdm(data_iterator):\n",
    "\n",
    "            input_ids = x['input_ids']\n",
    "            input_mask = x['input_mask']\n",
    "            segment_ids = x['segment_ids']\n",
    "            label = x['label']\n",
    "            filename_id = x[\"filename_id\"]\n",
    "            ind = x[\"ind\"]\n",
    "\n",
    "            ori_len = len(input_ids)\n",
    "            for each_item in [input_ids, input_mask, segment_ids]:\n",
    "                # 检查输入数据的长度，并在必要时对其进行填充，以确保每个批次的数据大小都与模型期望的一致\n",
    "                if ori_len < conf.batch_size_test:\n",
    "                    each_len = len(each_item[0])\n",
    "                    pad_x = [0] * each_len\n",
    "                    each_item += [pad_x] * (conf.batch_size_test - ori_len)\n",
    "\n",
    "            input_ids = torch.tensor(input_ids).to(conf.device)\n",
    "            input_mask = torch.tensor(input_mask).to(conf.device)\n",
    "            segment_ids = torch.tensor(segment_ids).to(conf.device)\n",
    "\n",
    "            # 使用模型进行预测，得到logits（模型的原始输出，通常在应用softmax之前的值）\n",
    "            logits = model(True, input_ids, input_mask,\n",
    "                           segment_ids, device=conf.device)\n",
    "\n",
    "            # 将所有预测结果、文件名标识符、索引存储起来\n",
    "            all_logits.extend(logits.tolist())\n",
    "            all_filename_id.extend(filename_id)\n",
    "            all_ind.extend(ind)\n",
    "\n",
    "    output_prediction_file = os.path.join(ksave_dir_mode,\n",
    "                                          \"predictions.json\")\n",
    "    \n",
    "    # 预测结果被写入到predictions.json文件中\n",
    "    if mode == \"valid\":\n",
    "        print_res = retrieve_evaluate(\n",
    "            all_logits, all_filename_id, all_ind, output_prediction_file, conf.valid_file, topn=conf.topn)\n",
    "    else:\n",
    "        print_res = retrieve_evaluate(\n",
    "            all_logits, all_filename_id, all_ind, output_prediction_file, conf.test_file, topn=conf.topn)\n",
    "\n",
    "    write_log(log_file, print_res)\n",
    "    print(print_res)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/peterchen/FinQA/code/retriever/1.ipynb 单元格 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m this_logits \u001b[39m=\u001b[39m model(\u001b[39mTrue\u001b[39;49;00m, input_ids, input_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m                     segment_ids, device\u001b[39m=\u001b[39;49mconf\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(\"complete inference: \", time.time() - start_time)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# start_time = time.time()        \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 计算损失\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m this_loss \u001b[39m=\u001b[39m criterion(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     this_logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, this_logits\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), label\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:161\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule(\u001b[39m*\u001b[39minputs[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs[\u001b[39m0\u001b[39m])\n\u001b[1;32m    160\u001b[0m replicas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplicate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids[:\u001b[39mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 161\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparallel_apply(replicas, inputs, kwargs)\n\u001b[1;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather(outputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:171\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparallel_apply\u001b[39m(\u001b[39mself\u001b[39m, replicas, inputs, kwargs):\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m parallel_apply(replicas, inputs, kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids[:\u001b[39mlen\u001b[39;49m(replicas)])\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py:78\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m         thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     77\u001b[0m     \u001b[39mfor\u001b[39;00m thread \u001b[39min\u001b[39;00m threads:\n\u001b[0;32m---> 78\u001b[0m         thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     _worker(\u001b[39m0\u001b[39m, modules[\u001b[39m0\u001b[39m], inputs[\u001b[39m0\u001b[39m], kwargs_tup[\u001b[39m0\u001b[39m], devices[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/threading.py:1011\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1012\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/threading.py:1027\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[39mif\u001b[39;00m lock \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1027\u001b[0m \u001b[39melif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1028\u001b[0m     lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1029\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in train_iterator:\n",
    "            \n",
    "    # print(\"current batch: \", k)\n",
    "    # start_time = time.time()\n",
    "\n",
    "    input_ids = torch.tensor(x['input_ids']).to(conf.device)\n",
    "    input_mask = torch.tensor(x['input_mask']).to(conf.device)\n",
    "    segment_ids = torch.tensor(x['segment_ids']).to(conf.device)\n",
    "    label = torch.tensor(x['label']).to(conf.device)\n",
    "\n",
    "    # 清除之前的梯度\n",
    "    model.zero_grad()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    this_logits = model(True, input_ids, input_mask,\n",
    "                        segment_ids, device=conf.device)\n",
    "\n",
    "    # print(\"complete inference: \", time.time() - start_time)\n",
    "    # start_time = time.time()        \n",
    "\n",
    "    # 计算损失\n",
    "    this_loss = criterion(\n",
    "        this_logits.view(-1, this_logits.shape[-1]), label.view(-1))\n",
    "\n",
    "    this_loss = this_loss.sum()\n",
    "    record_loss += this_loss.item() * 100\n",
    "    record_k += 1\n",
    "    k += 1\n",
    "\n",
    "    # 执行反向传播，更新模型参数\n",
    "    this_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print(\"complete backward: \", time.time() - start_time)\n",
    "\n",
    "    if k > 1 and k % conf.report_loss == 0:\n",
    "        # 每处理一定数量的批次后，记录当前的平均损失\n",
    "        write_log(log_file, \"%d : loss = %.3f\" %\n",
    "                    (k, record_loss / record_k))\n",
    "        record_loss = 0.0\n",
    "        record_k = 0\n",
    "\n",
    "    if k > 1 and k % conf.report == 0:\n",
    "        # 每隔一定数量的批次，评估模型在验证集上的性能，并保存当前的模型\n",
    "        print(\"Round: \", k / conf.report)\n",
    "        model.eval()\n",
    "        cost_time = time.time() - start_time\n",
    "        write_log(log_file, \"%d : time = %.3f \" %\n",
    "                    (k // conf.report, cost_time))\n",
    "        start_time = time.time()\n",
    "        if k // conf.report >= 1:\n",
    "            print(\"Val test\")\n",
    "            # save model\n",
    "            saved_model_path_cnt = os.path.join(\n",
    "                saved_model_path, 'loads', str(k // conf.report))\n",
    "            os.makedirs(saved_model_path_cnt, exist_ok=True)\n",
    "            torch.save(model.state_dict(),\n",
    "                        saved_model_path_cnt + \"/model.pt\")\n",
    "            # .state_dict() 是一个包含整个模型的参数和持久化缓冲区（比如批量归一化的运行平均值）的Python字典\n",
    "            # .pt 或 .pth 文件是PyTorch的标准文件扩展名，用于保存模型的参数\n",
    "\n",
    "            results_path_cnt = os.path.join(\n",
    "                results_path, 'loads', str(k // conf.report))\n",
    "            os.makedirs(results_path_cnt, exist_ok=True)\n",
    "            validation_result = evaluate(\n",
    "                valid_examples, valid_features, model, results_path_cnt, 'valid')\n",
    "            write_log(log_file, validation_result)\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/peterchen/FinQA/code/retriever/1.ipynb 单元格 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m p \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/home/peterchen/FinQA/dataset/dev copy.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdory/home/peterchen/FinQA/code/retriever/1.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m json\u001b[39m.\u001b[39;49mloads(p)\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[39mdel\u001b[39;00m kw[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/miniconda3/envs/FinQA/lib/python3.8/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "p = '/home/peterchen/FinQA/dataset/dev copy.json'\n",
    "json.loads(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
